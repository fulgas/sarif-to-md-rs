name: Performance Benchmarks

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: write
  pull-requests: write
  pages: write
  id-token: write

env:
  CARGO_TERM_COLOR: always

jobs:
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6
        with:
          fetch-depth: 0

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Setup Rust cache
        uses: Swatinem/rust-cache@v2
        with:
          cache-on-failure: true

      - name: Download previous benchmark results
        uses: actions/cache@v4
        with:
          path: |
            target/criterion
            benchmark-history
          key: benchmark-cache-${{ github.ref_name }}-${{ github.sha }}
          restore-keys: |
            benchmark-cache-${{ github.ref_name }}-
            benchmark-cache-main-

      - name: Download main branch benchmarks
        if: github.event_name == 'pull_request'
        uses: actions/download-artifact@v4
        with:
          name: main-branch-benchmarks
          path: main-benchmarks
        continue-on-error: true

      - name: Check if benchmarks needed
        id: check
        run: |
          echo "SKIP_BENCHMARKS=false" >> $GITHUB_OUTPUT
          echo "REUSE_MAIN_BENCHMARKS=false" >> $GITHUB_OUTPUT

          if [ "${{ github.event_name }}" = "pull_request" ]; then
            # Check if we have recent main branch benchmarks
            if [ -f "main-benchmarks/benchmark-results.txt" ] && [ -d "main-benchmarks/criterion" ]; then
              echo "âœ… Found cached main branch benchmarks, will reuse them"
              echo "REUSE_MAIN_BENCHMARKS=true" >> $GITHUB_OUTPUT
              echo "NEED_PR_BENCHMARKS=true" >> $GITHUB_OUTPUT
            else
              echo "âš ï¸ No main branch benchmarks found, will run both main and PR benchmarks"
              echo "NEED_PR_BENCHMARKS=true" >> $GITHUB_OUTPUT
            fi
          else
            # For main branch, check if we need to run benchmarks
            CURRENT_COMMIT="${{ github.sha }}"
            if [ -f "benchmark-history/results-$CURRENT_COMMIT.json" ]; then
              echo "âœ… Benchmarks already exist for this commit, skipping"
              echo "SKIP_BENCHMARKS=true" >> $GITHUB_OUTPUT
            else
              echo "ðŸ“Š Running benchmarks for new commit on main"
              echo "NEED_MAIN_BENCHMARKS=true" >> $GITHUB_OUTPUT
            fi
          fi

      - name: Run main branch baseline (cached or new)
        if: steps.check.outputs.REUSE_MAIN_BENCHMARKS == 'false' && (github.event_name == 'pull_request' || steps.check.outputs.NEED_MAIN_BENCHMARKS == 'true')
        run: |
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            # Switch to main branch for baseline
            git fetch origin main:main
            git checkout main
            echo "ðŸ”„ Running main branch benchmarks (this will be cached for future PRs)..."
            cargo bench --bench benchmarks -- --save-baseline main | tee main-benchmark-results.txt
            mkdir -p main-benchmarks
            cp main-benchmark-results.txt main-benchmarks/benchmark-results.txt
            cp -r target/criterion main-benchmarks/criterion
            git checkout ${{ github.event.pull_request.head.sha }}
          else
            echo "ðŸ“Š Running main branch benchmarks..."
            cargo bench --bench benchmarks | tee benchmark-results.txt
          fi

      - name: Run PR benchmarks and comparison
        if: steps.check.outputs.NEED_PR_BENCHMARKS == 'true' && steps.check.outputs.SKIP_BENCHMARKS == 'false'
        run: |
          echo "ðŸ” Running PR benchmarks and comparing against main..."

          # Load main branch baseline
          if [ "${{ steps.check.outputs.REUSE_MAIN_BENCHMARKS }}" = "true" ]; then
            echo "Using cached main branch benchmarks"
            cp -r main-benchmarks/criterion/* target/criterion/
            # Extract baseline from cached results
            cargo bench --bench benchmarks -- --load-baseline main | tee pr-benchmark-results.txt || true
          fi

          # Run PR benchmarks with comparison
          cargo bench --bench benchmarks -- --baseline main | tee benchmark-results.txt

          # Save comparison summary
          if grep -q "change:" benchmark-results.txt; then
            echo "PERFORMANCE_CHANGES_DETECTED=true" >> $GITHUB_ENV
            grep -A2 -B2 "change:" benchmark-results.txt > benchmark-summary.txt
          else
            echo "PERFORMANCE_CHANGES_DETECTED=false" >> $GITHUB_ENV
          fi

      - name: Process main branch results
        if: github.ref == 'refs/heads/main' && steps.check.outputs.SKIP_BENCHMARKS == 'false'
        run: |
          echo "ðŸ“Š Processing main branch benchmark results..."

          # Save current results for future comparisons
          CURRENT_COMMIT="${{ github.sha }}"
          mkdir -p benchmark-history
          cp -r target/criterion benchmark-history/criterion-$CURRENT_COMMIT

          # Extract key metrics to a JSON file
          cat > benchmark-history/results-$CURRENT_COMMIT.json << EOF
          {
            "commit": "$CURRENT_COMMIT",
            "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
            "branch": "${{ github.ref_name }}"
          }
          EOF

          # Generate historical trend report
          cat > performance-trend.md << 'EOF'
          # ðŸ“ˆ Performance Trend Analysis

          ## Recent Performance History

          | Commit | Date | Status | Key Changes |
          |--------|------|---------|-------------|
          EOF

          # Add last 10 commits to trend report
          for i in {0..9}; do
            COMMIT_HASH=$(git rev-parse HEAD~$i 2>/dev/null || break)
            COMMIT_DATE=$(git show -s --format=%cd --date=short HEAD~$i 2>/dev/null || break)
            COMMIT_MSG=$(git show -s --format=%s HEAD~$i 2>/dev/null | cut -c1-50 || break)

            if [ -f "benchmark-history/results-$COMMIT_HASH.json" ]; then
              echo "| \`${COMMIT_HASH:0:8}\` | $COMMIT_DATE | âœ… Benchmarked | $COMMIT_MSG |" >> performance-trend.md
            else
              echo "| \`${COMMIT_HASH:0:8}\` | $COMMIT_DATE | âšª No data | $COMMIT_MSG |" >> performance-trend.md
            fi
          done

          # Performance change detection (comparison with previous results)
          echo "" >> performance-trend.md
          echo "## Performance Status" >> performance-trend.md

          if grep -qi "slower\|regression\|degraded" benchmark-results.txt 2>/dev/null; then
            echo "âš ï¸ **Performance regression detected in this commit!**" >> performance-trend.md
            echo "PERFORMANCE_REGRESSION=true" >> $GITHUB_ENV
          elif grep -qi "faster\|improvement\|improved" benchmark-results.txt 2>/dev/null; then
            echo "ðŸš€ **Performance improvement detected!**" >> performance-trend.md
            echo "PERFORMANCE_IMPROVEMENT=true" >> $GITHUB_ENV
          else
            echo "âœ… **No significant performance changes detected.**" >> performance-trend.md
            echo "PERFORMANCE_STABLE=true" >> $GITHUB_ENV
          fi

      - name: Upload main branch benchmarks as artifact
        if: github.ref == 'refs/heads/main' && steps.check.outputs.SKIP_BENCHMARKS == 'false'
        uses: actions/upload-artifact@v4
        with:
          name: main-branch-benchmarks
          path: |
            benchmark-results.txt
            target/criterion
          retention-days: 30
          overwrite: true


      - name: Generate benchmark summary for PR
        if: github.event_name == 'pull_request'
        run: |
          echo "## ðŸ“Š Performance Benchmark Results vs Main Branch" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Check if we have performance changes
          if [ "$PERFORMANCE_CHANGES_DETECTED" = "true" ] && [ -f "benchmark-summary.txt" ]; then
            echo "### ðŸ” Performance Changes Detected" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`diff" >> $GITHUB_STEP_SUMMARY
            cat benchmark-summary.txt >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY

            # Add interpretation
            if grep -qi "faster\|improved" benchmark-summary.txt; then
              echo "âœ… **Overall performance improvement detected!**" >> $GITHUB_STEP_SUMMARY
            elif grep -qi "slower\|regression" benchmark-summary.txt; then
              echo "âš ï¸ **Performance regression detected - please review**" >> $GITHUB_STEP_SUMMARY
            else
              echo "â„¹ï¸ **Mixed performance changes - review individual benchmarks**" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "âœ… **No significant performance changes detected**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Performance is stable compared to main branch." >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ”— [View detailed benchmark reports](https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}/benchmarks/)"

      - name: Generate benchmark summary for main branch
        if: github.ref == 'refs/heads/main'
        run: |
          echo "## ðŸ“Š Performance Tracking Update" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Benchmark results for commit \`${{ github.sha }}\` have been recorded." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Show performance status
          if [ "$PERFORMANCE_REGRESSION" = "true" ]; then
            echo "âš ï¸ **Performance Regression Alert**" >> $GITHUB_STEP_SUMMARY
            echo "This commit introduced performance regressions. Please investigate." >> $GITHUB_STEP_SUMMARY
          elif [ "$PERFORMANCE_IMPROVEMENT" = "true" ]; then
            echo "ðŸš€ **Performance Improvement**" >> $GITHUB_STEP_SUMMARY
            echo "Great! This commit improved performance." >> $GITHUB_STEP_SUMMARY
          else
            echo "âœ… **Performance Stable**" >> $GITHUB_STEP_SUMMARY
            echo "No significant performance changes detected." >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Historical Tracking" >> $GITHUB_STEP_SUMMARY
          echo "- Results saved for future commit comparisons" >> $GITHUB_STEP_SUMMARY
          echo "- Available for PR baseline comparisons" >> $GITHUB_STEP_SUMMARY
          echo "- Historical trends published to GitHub Pages" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ”— [View performance dashboard](https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}/benchmarks/)"


  competitive-benchmark:
    name: Competitive Benchmarks
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    needs: benchmark

    strategy:
      matrix:
        tool:
          - name: microsoft-sarif-multitool
            install: npm install -g @microsoft/sarif-multitool
            command: sarif transform
            args: --pretty-print
            output_ext: json
          - name: sarif-om-python
            install: pip3 install sarif-om
            command: python3
            args: /tmp/sarif_om_test.py
            output_ext: json
          - name: jq-baseline
            install: echo 'jq already available'
            command: jq
            args: .
            output_ext: json

    steps:
      - name: Checkout repository
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6

      - name: Install tool - ${{ matrix.tool.name }}
        run: |
          echo "Installing ${{ matrix.tool.name }}..."
          ${{ matrix.tool.install }} || echo "Installation failed, will be marked as unavailable"

      - name: Verify tool installation
        id: verify
        run: |
          case "${{ matrix.tool.name }}" in
            "microsoft-sarif-multitool")
              if sarif --version >/dev/null 2>&1; then
                echo "available=true" >> $GITHUB_OUTPUT
              else
                echo "available=false" >> $GITHUB_OUTPUT
              fi
              ;;
            "sarif-om-python")
              if python3 -c "import sarif_om" >/dev/null 2>&1; then
                echo "available=true" >> $GITHUB_OUTPUT
                # Create test script for sarif-om
                cat > /tmp/sarif_om_test.py << 'EOF'
          import sys
          import json
          from sarif_om import SarifLog
          import time

          start_time = time.time()
          try:
              with open(sys.argv[1], 'r') as f:
                  sarif_log = SarifLog.from_json(json.load(f))

              output = sarif_log.to_json()
              with open(sys.argv[2], 'w') as f:
                  json.dump(output, f)

              end_time = time.time()
              print(f"{end_time - start_time:.3f}")
          except Exception as e:
              print("ERROR")
              sys.exit(1)
          EOF
              else
                echo "available=false" >> $GITHUB_OUTPUT
              fi
              ;;
            "jq-baseline")
              if command -v jq >/dev/null 2>&1; then
                echo "available=true" >> $GITHUB_OUTPUT
              else
                echo "available=false" >> $GITHUB_OUTPUT
              fi
              ;;
          esac

      - name: Run benchmarks for ${{ matrix.tool.name }}
        if: steps.verify.outputs.available == 'true'
        run: |
          mkdir -p results

          echo "# Benchmark Results for ${{ matrix.tool.name }}" > results/${{ matrix.tool.name }}-results.md
          echo "" >> results/${{ matrix.tool.name }}-results.md
          echo "| File | Input Size | Time | Output Size | Status |" >> results/${{ matrix.tool.name }}-results.md
          echo "|------|------------|------|-------------|---------|" >> results/${{ matrix.tool.name }}-results.md

          for sarif_file in examples/sarif-files/*.sarif; do
            if [ -f "$sarif_file" ]; then
              filename=$(basename "$sarif_file")
              filesize=$(wc -c < "$sarif_file")

              echo -n "| $filename | ${filesize}B | " >> results/${{ matrix.tool.name }}-results.md

              # Run benchmark with timeout
              case "${{ matrix.tool.name }}" in
                "microsoft-sarif-multitool")
                  if time_output=$(/usr/bin/time -f "%e" timeout 30s ${{ matrix.tool.command }} "$sarif_file" ${{ matrix.tool.args }} -o "/tmp/${{ matrix.tool.name }}-$filename.${{ matrix.tool.output_ext }}" 2>&1); then
                    time_taken=$(echo "$time_output" | tail -1)
                    output_size=$(wc -c < "/tmp/${{ matrix.tool.name }}-$filename.${{ matrix.tool.output_ext }}" 2>/dev/null || echo "0")
                    echo "${time_taken}s | ${output_size}B | âœ… |" >> results/${{ matrix.tool.name }}-results.md
                  else
                    echo "TIMEOUT | N/A | âš ï¸ |" >> results/${{ matrix.tool.name }}-results.md
                  fi
                  ;;
                "sarif-om-python")
                  if time_result=$(timeout 30s ${{ matrix.tool.command }} ${{ matrix.tool.args }} "$sarif_file" "/tmp/${{ matrix.tool.name }}-$filename.${{ matrix.tool.output_ext }}" 2>/dev/null); then
                    if [ "$time_result" != "ERROR" ]; then
                      output_size=$(wc -c < "/tmp/${{ matrix.tool.name }}-$filename.${{ matrix.tool.output_ext }}" 2>/dev/null || echo "0")
                      echo "${time_result}s | ${output_size}B | âœ… |" >> results/${{ matrix.tool.name }}-results.md
                    else
                      echo "ERROR | N/A | âŒ |" >> results/${{ matrix.tool.name }}-results.md
                    fi
                  else
                    echo "TIMEOUT | N/A | âš ï¸ |" >> results/${{ matrix.tool.name }}-results.md
                  fi
                  ;;
                "jq-baseline")
                  if time_output=$(/usr/bin/time -f "%e" timeout 10s ${{ matrix.tool.command }} ${{ matrix.tool.args }} "$sarif_file" > "/tmp/${{ matrix.tool.name }}-$filename.${{ matrix.tool.output_ext }}" 2>&1); then
                    time_taken=$(echo "$time_output" | tail -1)
                    output_size=$(wc -c < "/tmp/${{ matrix.tool.name }}-$filename.${{ matrix.tool.output_ext }}" 2>/dev/null || echo "0")
                    echo "${time_taken}s | ${output_size}B | âœ… |" >> results/${{ matrix.tool.name }}-results.md
                  else
                    echo "ERROR | N/A | âŒ |" >> results/${{ matrix.tool.name }}-results.md
                  fi
                  ;;
              esac
            fi
          done

      - name: Generate unavailable notice
        if: steps.verify.outputs.available == 'false'
        run: |
          mkdir -p results
          echo "# ${{ matrix.tool.name }} - Not Available" > results/${{ matrix.tool.name }}-results.md
          echo "" >> results/${{ matrix.tool.name }}-results.md
          echo "Tool installation failed or not compatible with this environment." >> results/${{ matrix.tool.name }}-results.md

      - name: Upload individual results
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.tool.name }}-results
          path: results/${{ matrix.tool.name }}-results.md

  combine-results:
    name: Combine Competitive Results
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    needs: [benchmark, competitive-benchmark]

    steps:
      - name: Checkout repository
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6

      - name: Download all competitive results
        uses: actions/download-artifact@v4
        with:
          pattern: "*-results"
          merge-multiple: true
          path: competitive-results

      - name: Download main benchmark results
        uses: actions/download-artifact@v4
        with:
          name: main-branch-benchmarks
          path: main-benchmarks
        continue-on-error: true

      - name: Combine all results
        run: |
          mkdir -p pages-content

          # Copy main benchmark results if available
          if [ -d "main-benchmarks/criterion" ]; then
            cp -r main-benchmarks/criterion/* pages-content/ || mkdir -p pages-content/report
          fi

          # Combine competitive results
          echo "# Competitive Performance Analysis" > pages-content/comparison.md
          echo "" >> pages-content/comparison.md
          echo "Benchmarking sarif-to-md-rs against other SARIF processing tools." >> pages-content/comparison.md
          echo "" >> pages-content/comparison.md
          echo "**Test Environment:** Ubuntu Latest, $(nproc) CPU cores" >> pages-content/comparison.md
          echo "**Date:** $(date)" >> pages-content/comparison.md
          echo "" >> pages-content/comparison.md

          # Add results from each tool
          for result_file in competitive-results/*-results.md; do
            if [ -f "$result_file" ]; then
              echo "" >> pages-content/comparison.md
              cat "$result_file" >> pages-content/comparison.md
              echo "" >> pages-content/comparison.md
            fi
          done

          # Add summary
          echo "## Summary" >> pages-content/comparison.md
          echo "" >> pages-content/comparison.md
          echo "**Key Findings:**" >> pages-content/comparison.md
          echo "- sarif-to-md-rs focuses on Markdown generation" >> pages-content/comparison.md
          echo "- Each tool serves different purposes in the SARIF ecosystem" >> pages-content/comparison.md
          echo "- Performance varies by file complexity and tool capabilities" >> pages-content/comparison.md

          # Create minimal theme index.html (same as before)
          cat > pages-content/index.html << 'EOF'
          <!DOCTYPE html>
          <html>
          <head>
            <title>SARIF to Markdown Performance Benchmarks</title>
            <style>
              body {
                font-family: 'SF Mono', Monaco, 'Cascadia Code', 'Roboto Mono', Consolas, 'Courier New', monospace;
                margin: 0;
                padding: 60px 40px;
                background-color: #ffffff;
                color: #333333;
                line-height: 1.6;
              }
              .container { max-width: 900px; margin: 0 auto; }
              h1 {
                color: #000000;
                font-size: 2em;
                font-weight: 300;
                margin-bottom: 2em;
                text-align: center;
                letter-spacing: -0.5px;
              }
              h2 {
                color: #333333;
                font-size: 1.2em;
                font-weight: 400;
                margin: 3em 0 1em 0;
                border-left: 3px solid #000000;
                padding-left: 1em;
              }
              a {
                color: #000000;
                text-decoration: none;
                border-bottom: 1px solid #cccccc;
                transition: border-bottom-color 0.2s;
              }
              a:hover { border-bottom-color: #000000; }
              .section { margin: 2em 0; padding: 0; }
              ul { list-style: none; padding: 0; }
              li {
                margin: 0.8em 0;
                padding-left: 2em;
                position: relative;
              }
              li:before {
                content: "â†’";
                position: absolute;
                left: 0;
                color: #666666;
              }
              .header-info {
                text-align: center;
                color: #666666;
                font-size: 0.9em;
                margin-bottom: 4em;
              }
              .divider {
                border: none;
                border-top: 1px solid #e0e0e0;
                margin: 4em 0;
              }
              .grid {
                display: grid;
                grid-template-columns: 1fr 1fr;
                gap: 4em;
                margin: 3em 0;
              }
              @media (max-width: 768px) {
                .grid { grid-template-columns: 1fr; gap: 2em; }
              }
            </style>
          </head>
          <body>
            <div class="container">
              <h1>SARIF to Markdown / Performance Benchmarks</h1>

              <div class="header-info">
                Updated $(date +"%B %d, %Y")<br>
                <a href="https://github.com/${{ github.repository }}">${{ github.repository }}</a>
              </div>

              <div class="grid">
                <div class="section">
                  <h2>Internal Benchmarks</h2>
                  <ul>
                    <li><a href="report/">Criterion Performance Reports</a></li>
                    <li><a href="report/parse_sarif/">SARIF Parsing</a></li>
                    <li><a href="report/generate_markdown/">Markdown Generation</a></li>
                    <li><a href="report/memory_usage/">Memory Analysis</a></li>
                  </ul>
                </div>

                <div class="section">
                  <h2>Competitive Analysis</h2>
                  <ul>
                    <li><a href="comparison.md">Full Comparison Report</a></li>
                    <li>Matrix-based testing across tools</li>
                    <li>Multiple SARIF file sizes tested</li>
                    <li>Performance across different scenarios</li>
                  </ul>
                </div>
              </div>

              <hr class="divider">

              <div class="section">
                <h2>Links</h2>
                <ul>
                  <li><a href="https://github.com/${{ github.repository }}">Source Repository</a></li>
                  <li><a href="https://github.com/${{ github.repository }}/actions/workflows/benchmarks.yml">CI Workflow</a></li>
                  <li><a href="https://github.com/${{ github.repository }}/releases">Releases</a></li>
                  <li><a href="https://crates.io/crates/sarif-to-md">Crates.io Package</a></li>
                </ul>
              </div>
            </div>
          </body>
          </html>
          EOF

      - name: Upload combined results
        uses: actions/upload-pages-artifact@v3
        with:
          path: pages-content

  deploy-pages:
    name: Deploy to GitHub Pages
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    needs: combine-results

    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4