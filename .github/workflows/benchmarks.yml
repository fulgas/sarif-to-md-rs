name: Performance Benchmarks

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: write
  pull-requests: write
  pages: write
  id-token: write

env:
  CARGO_TERM_COLOR: always

jobs:
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6
        with:
          fetch-depth: 0

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Setup Rust cache
        uses: Swatinem/rust-cache@v2
        with:
          cache-on-failure: true

      - name: Download previous benchmark results
        uses: actions/cache@v4
        with:
          path: |
            target/criterion
            benchmark-history
          key: benchmark-cache-${{ github.ref_name }}-${{ github.sha }}
          restore-keys: |
            benchmark-cache-${{ github.ref_name }}-
            benchmark-cache-main-

      - name: Download main branch benchmarks
        if: github.event_name == 'pull_request'
        uses: actions/download-artifact@v4
        with:
          name: main-branch-benchmarks
          path: main-benchmarks
        continue-on-error: true

      - name: Check if benchmarks needed
        id: check
        run: |
          echo "SKIP_BENCHMARKS=false" >> $GITHUB_OUTPUT
          echo "REUSE_MAIN_BENCHMARKS=false" >> $GITHUB_OUTPUT

          if [ "${{ github.event_name }}" = "pull_request" ]; then
            # Check if we have recent main branch benchmarks
            if [ -f "main-benchmarks/benchmark-results.txt" ] && [ -d "main-benchmarks/criterion" ]; then
              echo "‚úÖ Found cached main branch benchmarks, will reuse them"
              echo "REUSE_MAIN_BENCHMARKS=true" >> $GITHUB_OUTPUT
              echo "NEED_PR_BENCHMARKS=true" >> $GITHUB_OUTPUT
            else
              echo "‚ö†Ô∏è No main branch benchmarks found, will run both main and PR benchmarks"
              echo "NEED_PR_BENCHMARKS=true" >> $GITHUB_OUTPUT
            fi
          else
            # For main branch, check if we need to run benchmarks
            CURRENT_COMMIT="${{ github.sha }}"
            if [ -f "benchmark-history/results-$CURRENT_COMMIT.json" ]; then
              echo "‚úÖ Benchmarks already exist for this commit, skipping"
              echo "SKIP_BENCHMARKS=true" >> $GITHUB_OUTPUT
            else
              echo "üìä Running benchmarks for new commit on main"
              echo "NEED_MAIN_BENCHMARKS=true" >> $GITHUB_OUTPUT
            fi
          fi

      - name: Run main branch baseline (cached or new)
        if: steps.check.outputs.REUSE_MAIN_BENCHMARKS == 'false' && (github.event_name == 'pull_request' || steps.check.outputs.NEED_MAIN_BENCHMARKS == 'true')
        run: |
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            # Switch to main branch for baseline
            git fetch origin main:main
            git checkout main
            echo "üîÑ Running main branch benchmarks (this will be cached for future PRs)..."
            cargo bench --bench benchmarks -- --save-baseline main | tee main-benchmark-results.txt
            mkdir -p main-benchmarks
            cp main-benchmark-results.txt main-benchmarks/benchmark-results.txt
            cp -r target/criterion main-benchmarks/criterion
            git checkout ${{ github.event.pull_request.head.sha }}
          else
            echo "üìä Running main branch benchmarks..."
            cargo bench --bench benchmarks | tee benchmark-results.txt
          fi

      - name: Run PR benchmarks and comparison
        if: steps.check.outputs.NEED_PR_BENCHMARKS == 'true' && steps.check.outputs.SKIP_BENCHMARKS == 'false'
        run: |
          echo "üîç Running PR benchmarks and comparing against main..."

          # Load main branch baseline
          if [ "${{ steps.check.outputs.REUSE_MAIN_BENCHMARKS }}" = "true" ]; then
            echo "Using cached main branch benchmarks"
            cp -r main-benchmarks/criterion/* target/criterion/
            # Extract baseline from cached results
            cargo bench --bench benchmarks -- --load-baseline main | tee pr-benchmark-results.txt || true
          fi

          # Run PR benchmarks with comparison
          cargo bench --bench benchmarks -- --baseline main | tee benchmark-results.txt

          # Save comparison summary
          if grep -q "change:" benchmark-results.txt; then
            echo "PERFORMANCE_CHANGES_DETECTED=true" >> $GITHUB_ENV
            grep -A2 -B2 "change:" benchmark-results.txt > benchmark-summary.txt
          else
            echo "PERFORMANCE_CHANGES_DETECTED=false" >> $GITHUB_ENV
          fi

      - name: Process main branch results
        if: github.ref == 'refs/heads/main' && steps.check.outputs.SKIP_BENCHMARKS == 'false'
        run: |
          echo "üìä Processing main branch benchmark results..."

          # Save current results for future comparisons
          CURRENT_COMMIT="${{ github.sha }}"
          mkdir -p benchmark-history
          cp -r target/criterion benchmark-history/criterion-$CURRENT_COMMIT

          # Extract key metrics to a JSON file
          cat > benchmark-history/results-$CURRENT_COMMIT.json << EOF
          {
            "commit": "$CURRENT_COMMIT",
            "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
            "branch": "${{ github.ref_name }}"
          }
          EOF

          # Generate historical trend report
          cat > performance-trend.md << 'EOF'
          # üìà Performance Trend Analysis

          ## Recent Performance History

          | Commit | Date | Status | Key Changes |
          |--------|------|---------|-------------|
          EOF

          # Add last 10 commits to trend report
          for i in {0..9}; do
            COMMIT_HASH=$(git rev-parse HEAD~$i 2>/dev/null || break)
            COMMIT_DATE=$(git show -s --format=%cd --date=short HEAD~$i 2>/dev/null || break)
            COMMIT_MSG=$(git show -s --format=%s HEAD~$i 2>/dev/null | cut -c1-50 || break)

            if [ -f "benchmark-history/results-$COMMIT_HASH.json" ]; then
              echo "| \`${COMMIT_HASH:0:8}\` | $COMMIT_DATE | ‚úÖ Benchmarked | $COMMIT_MSG |" >> performance-trend.md
            else
              echo "| \`${COMMIT_HASH:0:8}\` | $COMMIT_DATE | ‚ö™ No data | $COMMIT_MSG |" >> performance-trend.md
            fi
          done

          # Performance change detection (comparison with previous results)
          echo "" >> performance-trend.md
          echo "## Performance Status" >> performance-trend.md

          if grep -qi "slower\|regression\|degraded" benchmark-results.txt 2>/dev/null; then
            echo "‚ö†Ô∏è **Performance regression detected in this commit!**" >> performance-trend.md
            echo "PERFORMANCE_REGRESSION=true" >> $GITHUB_ENV
          elif grep -qi "faster\|improvement\|improved" benchmark-results.txt 2>/dev/null; then
            echo "üöÄ **Performance improvement detected!**" >> performance-trend.md
            echo "PERFORMANCE_IMPROVEMENT=true" >> $GITHUB_ENV
          else
            echo "‚úÖ **No significant performance changes detected.**" >> performance-trend.md
            echo "PERFORMANCE_STABLE=true" >> $GITHUB_ENV
          fi

      - name: Upload main branch benchmarks as artifact
        if: github.ref == 'refs/heads/main' && steps.check.outputs.SKIP_BENCHMARKS == 'false'
        uses: actions/upload-artifact@v4
        with:
          name: main-branch-benchmarks
          path: |
            benchmark-results.txt
            target/criterion
          retention-days: 30
          overwrite: true


      - name: Generate benchmark summary for PR
        if: github.event_name == 'pull_request'
        run: |
          echo "## üìä Performance Benchmark Results vs Main Branch" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Check if we have performance changes
          if [ "$PERFORMANCE_CHANGES_DETECTED" = "true" ] && [ -f "benchmark-summary.txt" ]; then
            echo "### üîç Performance Changes Detected" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`diff" >> $GITHUB_STEP_SUMMARY
            cat benchmark-summary.txt >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY

            # Add interpretation
            if grep -qi "faster\|improved" benchmark-summary.txt; then
              echo "‚úÖ **Overall performance improvement detected!**" >> $GITHUB_STEP_SUMMARY
            elif grep -qi "slower\|regression" benchmark-summary.txt; then
              echo "‚ö†Ô∏è **Performance regression detected - please review**" >> $GITHUB_STEP_SUMMARY
            else
              echo "‚ÑπÔ∏è **Mixed performance changes - review individual benchmarks**" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "‚úÖ **No significant performance changes detected**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Performance is stable compared to main branch." >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "üîó [View detailed benchmark reports](https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}/benchmarks/)"

      - name: Generate benchmark summary for main branch
        if: github.ref == 'refs/heads/main'
        run: |
          echo "## üìä Performance Tracking Update" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Benchmark results for commit \`${{ github.sha }}\` have been recorded." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Show performance status
          if [ "$PERFORMANCE_REGRESSION" = "true" ]; then
            echo "‚ö†Ô∏è **Performance Regression Alert**" >> $GITHUB_STEP_SUMMARY
            echo "This commit introduced performance regressions. Please investigate." >> $GITHUB_STEP_SUMMARY
          elif [ "$PERFORMANCE_IMPROVEMENT" = "true" ]; then
            echo "üöÄ **Performance Improvement**" >> $GITHUB_STEP_SUMMARY
            echo "Great! This commit improved performance." >> $GITHUB_STEP_SUMMARY
          else
            echo "‚úÖ **Performance Stable**" >> $GITHUB_STEP_SUMMARY
            echo "No significant performance changes detected." >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Historical Tracking" >> $GITHUB_STEP_SUMMARY
          echo "- Results saved for future commit comparisons" >> $GITHUB_STEP_SUMMARY
          echo "- Available for PR baseline comparisons" >> $GITHUB_STEP_SUMMARY
          echo "- Historical trends published to GitHub Pages" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "üîó [View performance dashboard](https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}/benchmarks/)"


  competitive-benchmark:
    name: Competitive Benchmarks
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    needs: benchmark

    strategy:
      matrix:
        tool:
          - name: microsoft-sarif-multitool
            install: npm install -g @microsoft/sarif-multitool
            command: sarif transform
            args: --pretty-print
            output_ext: json
          - name: sarif-om-python
            install: pip3 install sarif-om
            command: python3
            args: /tmp/sarif_om_test.py
            output_ext: json
          - name: jq-baseline
            install: echo 'jq already available'
            command: jq
            args: .
            output_ext: json

    steps:
      - name: Checkout repository
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6

      - name: Install tool - ${{ matrix.tool.name }}
        run: |
          echo "Installing ${{ matrix.tool.name }}..."
          ${{ matrix.tool.install }} || echo "Installation failed, will be marked as unavailable"

      - name: Verify tool installation
        id: verify
        run: |
          case "${{ matrix.tool.name }}" in
            "microsoft-sarif-multitool")
              if sarif --version >/dev/null 2>&1; then
                echo "available=true" >> $GITHUB_OUTPUT
              else
                echo "available=false" >> $GITHUB_OUTPUT
              fi
              ;;
            "sarif-om-python")
              if python3 -c "import sarif_om" >/dev/null 2>&1; then
                echo "available=true" >> $GITHUB_OUTPUT
                # Create test script for sarif-om
                cat > /tmp/sarif_om_test.py << 'EOF'
          import sys
          import json
          from sarif_om import SarifLog
          import time

          start_time = time.time()
          try:
              with open(sys.argv[1], 'r') as f:
                  sarif_log = SarifLog.from_json(json.load(f))

              output = sarif_log.to_json()
              with open(sys.argv[2], 'w') as f:
                  json.dump(output, f)

              end_time = time.time()
              print(f"{end_time - start_time:.3f}")
          except Exception as e:
              print("ERROR")
              sys.exit(1)
          EOF
              else
                echo "available=false" >> $GITHUB_OUTPUT
              fi
              ;;
            "jq-baseline")
              if command -v jq >/dev/null 2>&1; then
                echo "available=true" >> $GITHUB_OUTPUT
              else
                echo "available=false" >> $GITHUB_OUTPUT
              fi
              ;;
          esac

      - name: Run benchmarks for ${{ matrix.tool.name }}
        if: steps.verify.outputs.available == 'true'
        run: |
          mkdir -p results

          echo "# Benchmark Results for ${{ matrix.tool.name }}" > results/${{ matrix.tool.name }}-results.md
          echo "" >> results/${{ matrix.tool.name }}-results.md
          echo "| File | Input Size | Time | Output Size | Status |" >> results/${{ matrix.tool.name }}-results.md
          echo "|------|------------|------|-------------|---------|" >> results/${{ matrix.tool.name }}-results.md

          for sarif_file in examples/sarif-files/*.sarif; do
            if [ -f "$sarif_file" ]; then
              filename=$(basename "$sarif_file")
              filesize=$(wc -c < "$sarif_file")

              echo -n "| $filename | ${filesize}B | " >> results/${{ matrix.tool.name }}-results.md

              # Run benchmark with timeout
              case "${{ matrix.tool.name }}" in
                "microsoft-sarif-multitool")
                  if time_output=$(/usr/bin/time -f "%e" timeout 30s ${{ matrix.tool.command }} "$sarif_file" ${{ matrix.tool.args }} -o "/tmp/${{ matrix.tool.name }}-$filename.${{ matrix.tool.output_ext }}" 2>&1); then
                    time_taken=$(echo "$time_output" | tail -1)
                    output_size=$(wc -c < "/tmp/${{ matrix.tool.name }}-$filename.${{ matrix.tool.output_ext }}" 2>/dev/null || echo "0")
                    echo "${time_taken}s | ${output_size}B | ‚úÖ |" >> results/${{ matrix.tool.name }}-results.md
                  else
                    echo "TIMEOUT | N/A | ‚ö†Ô∏è |" >> results/${{ matrix.tool.name }}-results.md
                  fi
                  ;;
                "sarif-om-python")
                  if time_result=$(timeout 30s ${{ matrix.tool.command }} ${{ matrix.tool.args }} "$sarif_file" "/tmp/${{ matrix.tool.name }}-$filename.${{ matrix.tool.output_ext }}" 2>/dev/null); then
                    if [ "$time_result" != "ERROR" ]; then
                      output_size=$(wc -c < "/tmp/${{ matrix.tool.name }}-$filename.${{ matrix.tool.output_ext }}" 2>/dev/null || echo "0")
                      echo "${time_result}s | ${output_size}B | ‚úÖ |" >> results/${{ matrix.tool.name }}-results.md
                    else
                      echo "ERROR | N/A | ‚ùå |" >> results/${{ matrix.tool.name }}-results.md
                    fi
                  else
                    echo "TIMEOUT | N/A | ‚ö†Ô∏è |" >> results/${{ matrix.tool.name }}-results.md
                  fi
                  ;;
                "jq-baseline")
                  if time_output=$(/usr/bin/time -f "%e" timeout 10s ${{ matrix.tool.command }} ${{ matrix.tool.args }} "$sarif_file" > "/tmp/${{ matrix.tool.name }}-$filename.${{ matrix.tool.output_ext }}" 2>&1); then
                    time_taken=$(echo "$time_output" | tail -1)
                    output_size=$(wc -c < "/tmp/${{ matrix.tool.name }}-$filename.${{ matrix.tool.output_ext }}" 2>/dev/null || echo "0")
                    echo "${time_taken}s | ${output_size}B | ‚úÖ |" >> results/${{ matrix.tool.name }}-results.md
                  else
                    echo "ERROR | N/A | ‚ùå |" >> results/${{ matrix.tool.name }}-results.md
                  fi
                  ;;
              esac
            fi
          done

      - name: Generate unavailable notice
        if: steps.verify.outputs.available == 'false'
        run: |
          mkdir -p results
          echo "# ${{ matrix.tool.name }} - Not Available" > results/${{ matrix.tool.name }}-results.md
          echo "" >> results/${{ matrix.tool.name }}-results.md
          echo "Tool installation failed or not compatible with this environment." >> results/${{ matrix.tool.name }}-results.md

      - name: Upload individual results
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.tool.name }}-results
          path: results/${{ matrix.tool.name }}-results.md

  combine-results:
    name: Combine Competitive Results
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    needs: [benchmark, competitive-benchmark]

    steps:
      - name: Checkout repository
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6

      - name: Download all competitive results
        uses: actions/download-artifact@v4
        with:
          pattern: "*-results"
          merge-multiple: true
          path: competitive-results

      - name: Download main benchmark results
        uses: actions/download-artifact@v4
        with:
          name: main-branch-benchmarks
          path: main-benchmarks
        continue-on-error: true

      - name: Combine all results
        run: |
          mkdir -p pages-content

          # Copy main benchmark results if available
          if [ -d "main-benchmarks/criterion" ]; then
            cp -r main-benchmarks/criterion pages-content/ || mkdir -p pages-content/criterion
          fi

          # Combine competitive results into markdown first
          echo "# Competitive Performance Analysis" > temp-comparison.md
          echo "" >> temp-comparison.md
          echo "Benchmarking sarif-to-md-rs against other SARIF processing tools." >> temp-comparison.md
          echo "" >> temp-comparison.md
          echo "**Test Environment:** Ubuntu Latest, $(nproc) CPU cores" >> temp-comparison.md
          echo "**Date:** $(date)" >> temp-comparison.md
          echo "" >> temp-comparison.md

          # Add results from each tool
          for result_file in competitive-results/*-results.md; do
            if [ -f "$result_file" ]; then
              echo "" >> temp-comparison.md
              cat "$result_file" >> temp-comparison.md
              echo "" >> temp-comparison.md
            fi
          done

          # Add summary
          echo "## Summary" >> temp-comparison.md
          echo "" >> temp-comparison.md
          echo "**Key Findings:**" >> temp-comparison.md
          echo "- sarif-to-md-rs focuses on Markdown generation" >> temp-comparison.md
          echo "- Each tool serves different purposes in the SARIF ecosystem" >> temp-comparison.md
          echo "- Performance varies by file complexity and tool capabilities" >> temp-comparison.md

          # Keep the markdown version
          cp temp-comparison.md pages-content/comparison.md

      - name: Install pandoc for markdown conversion
        run: |
          sudo apt-get update
          sudo apt-get install -y pandoc

      - name: Convert markdown to HTML with custom styling
        run: |

          # Add navigation header to markdown
          echo '<div class="back-link"><a href="index.html">‚Üê Back to Dashboard</a></div>' > pages-content/comparison-header.html

          # Convert markdown to HTML using pandoc with shared styles
          pandoc pages-content/comparison.md \
            --from markdown \
            --to html5 \
            --css shared-style.css \
            --include-before-body pages-content/comparison-header.html \
            --metadata title="Competitive Performance Analysis - SARIF to Markdown" \
            --standalone \
            --output pages-content/comparison.html

      - name: Generate index page content
        run: |
          # Create index markdown content
          cat > pages-content/index.md << 'EOF'
          ---
          title: "SARIF to Markdown Performance Benchmarks"
          ---

          # SARIF to Markdown / Performance Benchmarks

          <div class="header-info">
          Updated $(date +"%B %d, %Y")<br>
          <a href="https://github.com/${{ github.repository }}">${{ github.repository }}</a>
          </div>

          <div class="grid">
          <div class="section">

          ## Internal Benchmarks

          - [Core Generation](criterion/core_generation/report/)
          - [File Processing](criterion/file_processing/report/)
          - [Memory Usage](criterion/memory_usage/report/)
          - [Output Formats](criterion/output_formats/report/)
          - [SARIF Complexity](criterion/sarif_complexity/report/)

          </div>
          <div class="section">

          ## Competitive Analysis

          - [Full Comparison Report](comparison.html)
          - Matrix-based testing across tools
          - Multiple SARIF file sizes tested
          - Performance across different scenarios

          </div>
          </div>

          ---

          ## Links

          - [Source Repository](https://github.com/${{ github.repository }})
          - [CI Workflow](https://github.com/${{ github.repository }}/actions/workflows/benchmarks.yml)
          - [Releases](https://github.com/${{ github.repository }}/releases)
          - [Crates.io Package](https://crates.io/crates/sarif-to-md)

          EOF

          # Create shared CSS for both index and comparison pages
          cat > pages-content/shared-style.css << 'EOF'
          body {
            font-family: 'SF Mono', Monaco, 'Cascadia Code', 'Roboto Mono', Consolas, 'Courier New', monospace;
            margin: 0;
            padding: 60px 40px;
            background-color: #ffffff;
            color: #333333;
            line-height: 1.6;
            max-width: 900px;
            margin: 0 auto;
          }
          h1 {
            color: #000000;
            font-size: 2em;
            font-weight: 300;
            margin-bottom: 2em;
            text-align: center;
            letter-spacing: -0.5px;
          }
          h2 {
            color: #333333;
            font-size: 1.2em;
            font-weight: 400;
            margin: 3em 0 1em 0;
            border-left: 3px solid #000000;
            padding-left: 1em;
          }
          h3 {
            color: #555555;
            font-size: 1em;
            font-weight: 400;
            margin: 2em 0 1em 0;
          }
          a {
            color: #000000;
            text-decoration: none;
            border-bottom: 1px solid #cccccc;
            transition: border-bottom-color 0.2s;
          }
          a:hover { border-bottom-color: #000000; }
          ul {
            list-style: none;
            padding: 0;
          }
          li {
            margin: 0.8em 0;
            padding-left: 2em;
            position: relative;
          }
          li:before {
            content: "‚Üí";
            position: absolute;
            left: 0;
            color: #666666;
          }
          .header-info {
            text-align: center;
            color: #666666;
            font-size: 0.9em;
            margin-bottom: 4em;
          }
          .grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 4em;
            margin: 3em 0;
          }
          .section { margin: 2em 0; padding: 0; }
          hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 4em 0;
          }
          p { margin: 1em 0; }
          strong { font-weight: 600; }

          /* Table styles for comparison page */
          table {
            width: 100%;
            border-collapse: collapse;
            margin: 2em 0;
            font-size: 0.9em;
          }
          th, td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #e0e0e0;
          }
          th {
            background-color: #f8f8f8;
            font-weight: 600;
            color: #000000;
          }
          tr:hover {
            background-color: #f9f9f9;
          }
          .back-link {
            display: inline-block;
            margin-bottom: 2em;
            padding: 0.5em 1em;
            border: 1px solid #e0e0e0;
            background-color: #f8f8f8;
            text-decoration: none;
          }

          @media (max-width: 768px) {
            .grid { grid-template-columns: 1fr; gap: 2em; }
          }
          EOF

          # Convert index markdown to HTML
          pandoc pages-content/index.md \
            --from markdown \
            --to html5 \
            --css shared-style.css \
            --standalone \
            --output pages-content/index.html

      - name: Upload combined results
        uses: actions/upload-pages-artifact@v3
        with:
          path: pages-content

  deploy-pages:
    name: Deploy to GitHub Pages
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    needs: combine-results

    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4